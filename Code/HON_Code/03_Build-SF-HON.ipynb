{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have the ditribution file for each source and target pair\n",
    "How to merge multiple source and target probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/user/m/msaebi/Public/Code-SF-HON\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scripts import build_rules_utils as BRU\n",
    "from scripts import utils\n",
    "%cd /afs/crc.nd.edu/user/m/msaebi/Public/Code-SF-HON/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count = {}\n",
    "Rules = defaultdict(dict)\n",
    "Distribution = defaultdict(dict)\n",
    "final_dist=defaultdict(dict)\n",
    "SourceToExtSource = {}\n",
    "Verbose = True\n",
    "\n",
    "def Initialize():\n",
    "    Count = {}\n",
    "    Rules = defaultdict(dict)\n",
    "    Distribution = defaultdict(dict)\n",
    "    SourceToExtSource = {}\n",
    "\n",
    "def ExtractRules(Trajectory, MaxOrder, MinSupport):\n",
    "#    Initialize()\n",
    "    Build_Observations_Distributions(Trajectory)\n",
    "    Aggragate_Probs(Distribution,MinSupport)\n",
    "    GenerateAllRules(MaxOrder)\n",
    "    return Rules\n",
    "\n",
    "#*******************************************\n",
    "def Build_Observations_Distributions(Trajectory):\n",
    "#build distribution based on observation file. append secveral probabilities\n",
    "#cprresponding to the same source & target\n",
    "    \n",
    "    for record in Trajectory: #record=[movement, prob]\n",
    "        trajectory = record[0]     #Modified for no ship data record[1] \n",
    "        Target = trajectory[-1]\n",
    "        Source = tuple(trajectory[:-1])\n",
    "        prob=float(record[1])\n",
    "\n",
    "        if Source in Distribution:\n",
    "            if Target not in Distribution[Source]:\n",
    "                Distribution[Source].update({str(Target):[prob]})\n",
    "            if Target in Distribution[Source]:\n",
    "                Distribution[Source][Target].append(prob)                \n",
    "        else:\n",
    "            Distribution[Source][Target]=[prob]            \n",
    "    return(Distribution)\n",
    "\n",
    "\n",
    "def Aggragate_Probs(Distribution,MinSupport):\n",
    "#aggeregate all probabilities to each other\n",
    "    a=[]\n",
    "    for source,value in Distribution.items():#value={target:prob}\n",
    "        for target,val in value.items(): #key=target, val=prob\n",
    "            if (len(val)>1):\n",
    "                #a[:] = [1-x for x in val] #subtract all elemnts of the list from 1\n",
    "                #final_prob=1-functools.reduce(operator.mul, a, 1) #multiply them by each other and subtract the result form 1\n",
    "                final_prob=np.mean(val)\n",
    "                if (final_prob>MinSupport):\n",
    "                    final_dist[source][target]=float(final_prob)\n",
    "            else:\n",
    "                if (float(val[0])>MinSupport):\n",
    "                    final_dist[source][target]=float(val[0])\n",
    "    return(final_dist)\n",
    "\n",
    "#*******************************************\n",
    "def GenerateAllRules(MaxOrder):\n",
    "    print('building cache')\n",
    "    BuildSourceToExtSource() # to speed up lookups\n",
    "    print('generating rules')\n",
    "    #VPrint(len([x for x in final_dist if len(x) == 1]))\n",
    "    LoopCounter = 0\n",
    "    for Source in final_dist.copy():\n",
    "        if len(Source) == 1:\n",
    "            AddToRules(Source)\n",
    "            ExtendRule(Source, Source, 1, MaxOrder)\n",
    "            LoopCounter += 1\n",
    "    return Rules\n",
    "\n",
    "def ExtendRule(Valid, Curr, order, MaxOrder):\n",
    "    if order >= MaxOrder:\n",
    "        AddToRules(Valid)\n",
    "    else:\n",
    "        Distr = final_dist[Valid]\n",
    "        NewOrder = order + 1\n",
    "        Extended = ExtendSource(Curr, NewOrder)\n",
    "        if len(Extended) == 0:\n",
    "            AddToRules(Valid)\n",
    "        else:\n",
    "            for ExtSource in Extended:\n",
    "                ExtDistr = final_dist[ExtSource] # Pseudocode in Algorithm 1 has a typo here\n",
    "                if BRU.KLD(ExtDistr, Distr) > KLDThreshold(NewOrder, ExtSource):\n",
    "                    # higher-order dependencies exist for order NewOrder\n",
    "                    # keep comparing probability distribution of higher orders with current order\n",
    "                    ExtendRule(ExtSource, ExtSource, NewOrder, MaxOrder)\n",
    "                else:\n",
    "                    # higher-order dependencies do not exist for current order\n",
    "                    # keep comparing probability distribution of higher orders with known order\n",
    "                    ExtendRule(Valid, ExtSource, NewOrder, MaxOrder)\n",
    "\n",
    "def AddToRules(Source):\n",
    "    if len(Source) > 0:\n",
    "        ## To output frequencies instead of probabilities, change \"Distribution\" to \"Count\"\n",
    "        ## and filter out zero values\n",
    "        Rules[Source] = final_dist[Source]\n",
    "        PrevSource = Source[:-1]\n",
    "        AddToRules(PrevSource)\n",
    "\n",
    "###########################################\n",
    "# Auxiliary functions\n",
    "###########################################\n",
    "\n",
    "def ExtractSubSequences(trajectory, order):\n",
    "    SubSequence = []\n",
    "    for starting in range(len(trajectory) - order + 1):\n",
    "        SubSequence.append(tuple(trajectory[starting:starting + order]))\n",
    "    return SubSequence\n",
    "\n",
    "def IncreaseCounter(Source, Target):\n",
    "    if not Source in Count:\n",
    "        Count[Source] = Counter()\n",
    "    Count[Source][Target]+=1\n",
    "    return Count\n",
    "\n",
    "def ProdProb(Source, Target,Distribution):\n",
    "    if not Source in Count:\n",
    "        Count[Source] = Counter()\n",
    "    Count[Source][Target] += 1\n",
    "    return Count\n",
    "#*******************************************\n",
    "def ExtendSourceSlow(Curr, NewOrder):\n",
    "    Extended = []\n",
    "    for CandidateSource in final_dist:\n",
    "        if len(CandidateSource) == NewOrder and CandidateSource[-len(Curr):] == Curr:\n",
    "            Extended.append(CandidateSource)\n",
    "    return Extended\n",
    "\n",
    "def ExtendSource(Curr, NewOrder):\n",
    "    if Curr in SourceToExtSource:\n",
    "        if NewOrder in SourceToExtSource[Curr]:\n",
    "            return SourceToExtSource[Curr][NewOrder]\n",
    "    return []\n",
    "\n",
    "## creating a cache for fast lookup\n",
    "def BuildSourceToExtSource():\n",
    "    for source in final_dist:\n",
    "        if len(source) > 1:\n",
    "            NewOrder = len(source)\n",
    "            for starting in range(1, len(source)):\n",
    "                curr = source[starting:]\n",
    "                if not curr in SourceToExtSource:\n",
    "                    SourceToExtSource[curr] = {}\n",
    "                if not NewOrder in SourceToExtSource[curr]:\n",
    "                    SourceToExtSource[curr][NewOrder] = set()\n",
    "                SourceToExtSource[curr][NewOrder].add(source)\n",
    "def KLDThreshold(NewOrder, ExtSource):\n",
    "    #print(\"NewOrder\",NewOrder)\n",
    "    #print(\"ExtSource\",ExtSource)\n",
    "    return NewOrder / math.exp(1 + (sum(i for i in final_dist[ExtSource].values()))) # typo in Pseudocode in Algorithm 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "Graph = defaultdict(dict)\n",
    "Verbose = True\n",
    "\n",
    "def Initialize():\n",
    "    global Graph\n",
    "    Graph = defaultdict(dict)\n",
    "\n",
    "\n",
    "def BuildNetwork(Rules):\n",
    "    print('Building network')\n",
    "    Initialize()\n",
    "    SortedSource = sorted(Rules, key=lambda x: len(x))\n",
    "    for source in SortedSource:\n",
    "        for target in Rules[source]:\n",
    "            Graph[source][(target,)] = Rules[source][target]\n",
    "            # following operations are destructive to Rules\n",
    "            if len(source) > 1:\n",
    "                Rewire(source, (target,))\n",
    "    RewireTails()\n",
    "    return Graph\n",
    "\n",
    "def Rewire(source, target):\n",
    "    #print(source, target)\n",
    "    PrevSource = source[:-1]\n",
    "    PrevTarget = (source[-1],)\n",
    "    if not PrevSource in Graph or not source in Graph[PrevSource]:\n",
    "        try:\n",
    "            Graph[PrevSource][source] = Graph[PrevSource][PrevTarget]\n",
    "            del(Graph[PrevSource][PrevTarget])\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "def RewireTails():\n",
    "    ToAdd = []\n",
    "    ToRemove = []\n",
    "    for source in Graph:\n",
    "        for target in Graph[source]:\n",
    "            if len(target) == 1:\n",
    "                NewTarget = source + target\n",
    "                while len(NewTarget) > 1:\n",
    "                    if NewTarget in Graph:\n",
    "                        ToAdd.append((source, NewTarget, Graph[source][target]))\n",
    "                        ToRemove.append((source, target))\n",
    "                        break\n",
    "                    else:\n",
    "                        NewTarget = NewTarget[1:]\n",
    "    for (source, target, weight) in ToAdd:\n",
    "        Graph[source][target] = weight\n",
    "    for (source, target) in ToRemove:\n",
    "        del(Graph[source][target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='1997_2018'\n",
    "year='2018'\n",
    "y=year+'/'\n",
    "MaxOrder = 16\n",
    "MinSupport = 1e-20\n",
    "\n",
    "#Input:\n",
    "port_data='data/Places_allportdata_mergedSept2017.csv'\n",
    "#eco='noEco_';paul='';env='noEnv_'\n",
    "r='r0_'\n",
    "r=''\n",
    "env='env_' ;eco='Eco_' ;paul=''\n",
    "#eco='sameEco_';paul='zh/'+r+'/';env='env_'\n",
    "InputForHON_Ballast='data/'+y+paul+'l_InputForHON_Ballast_' +env+eco+ str(MaxOrder)+'.csv'\n",
    "InputForHON_Fouling='data/'+y+paul+'l_InputForHON_Fouling_' +env+eco+ str(MaxOrder)+'.csv'\n",
    "\n",
    "\n",
    "#output:\n",
    "OutputRulesFile_Ballast='data/'+y+paul+r+'Rules_Ballast_' +env+eco+ str(year)+'_'+str(MaxOrder)+'.csv'\n",
    "OutputNetworkFile_Ballast='data/'+y+paul+r+'HONet_Ballast_' +env+eco+ str(year)+'_'+str(MaxOrder)+'.csv'\n",
    "\n",
    "OutputRulesFile_Fouling='data/'+y+paul+'/l_Rules_Fouling_' +env+eco+ str(year)+'_'+str(MaxOrder)+'.csv'\n",
    "OutputNetworkFile_Fouling='data/'+y+paul+'/l_HONet_Fouling_' +env+eco+ str(year)+'_'+str(MaxOrder)+'.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading raw sequential data\n",
      "Building training and testing\n",
      "RAW Trajectory\n",
      "17575305\n",
      "Training Trajectory\n",
      "17575305\n",
      "Testing Trajectory\n",
      "17575305\n"
     ]
    }
   ],
   "source": [
    "#Choose biofouling or balast\n",
    "#InputFileName , OutputRulesFile , OutputNetworkFile= InputForHON_Fouling , OutputRulesFile_Fouling , OutputNetworkFile_Fouling\n",
    "InputFileName ,OutputRulesFile, OutputNetworkFile= InputForHON_Ballast , OutputRulesFile_Ballast, OutputNetworkFile_Ballast\n",
    "\n",
    "LastStepsHoldOutForTesting = 1\n",
    "MinimumLengthForTraining = 1\n",
    "InputFileDeliminator = ' '\n",
    "\n",
    "ports = utils.GetPortData(port_data, 'ID', ',')\n",
    "RawTrajectories = BRU.ReadSequentialData(InputFileName,InputFileDeliminator, MinimumLengthForTraining, LastStepsHoldOutForTesting)\n",
    "TrainingTrajectory, TestingTrajectory = BRU.BuildTrainingAndTesting(RawTrajectories,LastStepsHoldOutForTesting)\n",
    "\n",
    "print(\"RAW Trajectory\");BRU.VPrint(len(RawTrajectories))\n",
    "print(\"Training Trajectory\");BRU.VPrint(len(TrainingTrajectory))\n",
    "print(\"Testing Trajectory\");BRU.VPrint(len(TestingTrajectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building cache\n",
      "generating rules\n",
      "Building network\n",
      "Dumping rules to file\n",
      "Dumping network to file\n"
     ]
    }
   ],
   "source": [
    "Build_Observations_Distributions(RawTrajectories)\n",
    "Aggragate_Probs(Distribution,MinSupport)#eliminated 13.42 % of the paths(probs) 5935 of 6855. mean:0.00556928 Stdev 0.033\n",
    "Rules= GenerateAllRules(MaxOrder)\n",
    "Network = BuildNetwork(Rules)\n",
    "BRU.DumpRules(Rules, OutputRulesFile)\n",
    "BRU.DumpNetwork(Network, OutputNetworkFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic network stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "order=16\n",
    "r='r0'\n",
    "eco='sameEco_';paul='zh/'+r+'/';env='env_'\n",
    "HONet_translated_b='data/'+y+paul+r+'_trans_HONet_Ballast_'+ eco+env+year+'_'+str(order)+'.net'\n",
    "HONet_translated_f='data/'+y+paul+r+'_trans_HONet_Fouling_'+ eco+env+year+'_'+str(order)+'.net'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 14674\n",
      "Number of edges: 139474\n",
      "Average in degree:   9.5048\n",
      "Average out degree:   9.5048\n"
     ]
    }
   ],
   "source": [
    "f = pd.read_csv(OutputNetworkFile_Ballast, sep=',')\n",
    "f = pd.read_csv(HONet_translated_b, sep=' ')\n",
    "f.columns = ['source', 'target','weight']\n",
    "G = nx.from_pandas_edgelist(f,source='source', target='target',edge_attr='weight',create_using=nx.DiGraph())\n",
    "print(nx.info(G))\n",
    "#f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
